# 基座模型微调
本研究基于Meta公司于2023年发布的一款跨模态对齐的开源多模态大模型ImageBind[1]，为了将其应用于古琴减字谱这样高度专业的垂直领域，还参考了Github上的一项工作，该项工作探索了一条基于开源的DreamBooth数据集对ImageBind进行LoRA微调的路径[2]。

### 参考文献：
[1]ROHIT G, EL-NOUBY A,LIU Z,et al. ImageBind one embedding space to bind them all[C]//2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition,Vancouver,2023:15180-15190.
[2]RUIZ N,LI Y,JAMPANI V,et al. Dreambooth:fine tuning text-to-image diffusion models for subject-driven generation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Vancouver,2023:22500-22510.
### 项目网址：
https://github.com/fabawi/ImageBind-LoRA

## 实验一：
数据集：96个减字谱谱字（96个古琴减字谱样本）的图文对照数据（约500张图像），平均每个谱字约对应5张图像，图（谱字图像）-文（谱字名称）比为5:1，其中训练集和验证集的分配比例为8:2。
利用上述数据集，在学习率（learn rate, lr）超参设置为 5e-6、批次大小（Batch Size）超参设置为12、训练的模态为Text和Vision、硬件环境为Apple pro M3 48G CPU的情况下，微调训练总耗时为1.445天个34.68小时，共训练了200个轮次，得到的模型为GuQin_IB-LoRA-200。
微调结果如下：
train loss epoch: 2.2171 
train loss cross epoch: 1.6736
val acc top 1: 0.5556
val acc top 5: 0.8681

## 实验二：
采用与实验一同样的实验方案，但将训练数据集增加到减字谱图像约4000张，平均每个减字谱约对应50张图像，图（谱字图像）-文（谱字名称）比为50:1，其中训练集和验证集的分配比例仍为8:2。基于该数据集，在与实验一同样的学习率（learn rate, lr）超参设置为 5e-6、批次大小（Batch Size）超参设置为12、训练的模态为Text和Vision、而将硬件环境升级为NVIDA 4090D 24G的情况下，微调训练总耗时为12个小时，共训练了180个轮次，得到的模型为GuQin_IB-LoRA-180。
微调结果如下：
train loss epoch: 1.5672 ↓
train loss cross epoch：0.6305 ↓
acc top 1: 0.7884 ↑
acc top 5: 0.9538 ↑
实验二表明，增加训练数据集的规模和改善硬件环境，可以显著优化各项微调训练的指标，降低损失函数的值，提升模型在验证数据集上的精确度。

## 实验三：
采用与实验二同样的实验方案，硬件环境和超参设置不变，训练数据集的图文配对数据规模和比例不变，但在训练数据集中增加与谱字相对应音频，每个减字谱对应一个音频，训练的模态为Text、Vision和Audio三个模态。继续在实验二的基础上，微调训练总耗时12.18小时，共训练了220个轮次，得到的模型为GuQin_IB-LoRA-220，模型权重文件已上传至checkpoints目录下。
微调结果如下：
epoch loss : 1.5609 ↓
cross epoch loss：0.6304 ↓
acc top 1: 0.8042 ↑
acc top 5: 0.9666 ↑

96个古琴减字谱楷体字符样本如下：
![少样本数据集](https://github.com/xtykc/guqin/blob/main/%E5%8F%A4%E7%90%B4%E5%87%8F%E5%AD%97%E8%B0%B1%E5%86%85%E6%B5%8B%E6%95%B0%E6%8D%AE%E9%9B%86-96.jpg "96个复合谱字")
