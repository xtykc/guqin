# 基座模型大样本微调

## 数据集
  相比于小样本微调，减字谱的种数增加到19005个，涵盖了绝大部分古籍所载和当世所用古琴减字谱的基础谱字共1464个；图文比也从50:1扩大到了70:1，图像总量达到135万余张。

## 算力配备

采用了单机多卡和Deepspeed加速的方式进行训练，硬件环境为一台18核CPU的服务器上挂载8张24G的RTX 4090D的GPU，学习率（learn rate, lr）超参设置为 5e-6、训练的模态为Text和Vision。
采用分布式数据并行策略 (Distributed Data Parallel Strategy，DDPStrategy)。 每个GPU的批次大小（Batch Size）设置为12、数据加载时使用的子进程数（num_workers）设置为4，最终训练了120个轮次，共耗时4.947天，118.728个小时，得到的基座模型命名为“GuQin_IB-LoRA-120”。

## 训练结果

train loss epoch: 1.5422 
train loss cross epoch: 0.6275
val acc top 1: 0.8619
val acc top 5: 0.9858
